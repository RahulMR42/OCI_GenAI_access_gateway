- region: us-chicago-1
  compartment_id: ocid1.compartment.oc1..xxxxxxxxxxxxxxxxxxxxxxxxxxxxxxx
  models:
    ondemand:
      - name: cohere.command-r-plus-08-2024
        model_id: cohere.command-r-plus-08-2024
        description: "delivers roughly 50% higher throughput and 25% lower latencies as compared to the previous Command R+ version, while keeping the hardware footprint the same."

      - name: cohere.command-r-08-2024
        model_id: cohere.command-r-08-2024
        description: "delivers roughly 50% higher throughput and 25% lower latencies as compared to the previous Command R+ version, while keeping the hardware footprint the same."

      - name: cohere.command-r-plus
        model_id: cohere.command-r-plus
        description: "Chat models: Optimized for complex tasks, offers advanced language understanding, higher capacity, and more nuanced responses, and can maintain context from its long conversation history of 128,000 tokens. Also ideal for question-answering, sentiment analysis, and information retrieval."

      - name: cohere.command-r-16k
        model_id: cohere.command-r-16k
        description: "Optimized for conversational interaction and long context tasks. Ideal for text generation, summarization, translation, or text-based classification."


      - name: meta.llama-3.3-70b-instruct
        model_id: meta.llama-3.3-70b-instruct
        description: "Model has 70 billion parameters.Accepts text-only inputs and produces text-only outputs.Delivers better performance than both Llama 3.1 70B and Llama 3.2 90B for text tasks.Maximum prompt + response length 128,000 tokens for each run.For on-demand inferencing, the response length is capped at 4,000 tokens for each run."

      - name: meta.llama-3.2-11b-vision-instruct
        model_id: meta.llama-3.2-11b-vision-instruct
        description: "Model has 11 billion parameters.Dedicated mode only. (On-demand inferencing not available.) For dedicated inferencing, create a dedicated AI cluster and endpoint and host the model on the cluster.Context length 128,000 tokens.Maximum prompt + response length 128,000 tokens for each run.Multimodal support Input text and images and get a text output.English is the only supported language for the image plus text option.Multilingual option supported for the text only option."

      - name: meta.llama-3.2-90b-vision-instruct
        model_id: meta.llama-3.2-90b-vision-instruct
        description: "Model has 90 billion parameters.Context length: 128,000 tokens.Maximum prompt + response length: 128,000 tokens for each run.For on-demand inferencing, the response length is capped at 4,000 tokens for each run.Multimodal support: Input text and images and get a text output.English is the only supported language for the image plus text option.Multilingual option supported for the text only option."

      - name: meta.llama-3.1-70b-instruct
        model_id: meta.llama-3.1-70b-instruct
        description: "This 70 billion-parameter generation model is perfect for content creation, conversational AI, and enterprise applications."

      - name: meta.llama-3.1-405b-instruct
        model_id: meta.llama-3.1-405b-instruct
        description: "This 405 billion-parameter model is a high-performance option that offers speed and scalability."

      - name: meta.llama-3-70b-instruct
        model_id: meta.llama-3-70b-instruct
        description: "Model has 70 billion parameters.Maximum prompt + response length: 8,000 tokens for each run.Has a broad general knowledge, from generating ideas to refining text analysis and drafting written content, such as emails, blog posts, and descriptions."

    embedding:
      - name: cohere.embed-multilingual-v3.0
        model_id: cohere.embed-multilingual-v3.0
        description: "Provides multilingual classification and embedding support."

      - name: cohere.embed-multilingual-v3.1
        model_id: cohere.embed-multilingual-v3.1
        description: "Provides multilingual classification and embedding support."

      - name: cohere.embed-english-v3.0
        model_id: cohere.embed-english-v3.0
        description: "A model that allows for text to be classified or turned into embeddings. English only."

      - name: cohere.embed-english-v3.1
        model_id: cohere.embed-english-v3.1
        description: "A model that allows for text to be classified or turned into embeddings. English only."

      - name: cohere.embed-english-light-v3.0
        model_id: cohere.embed-english-light-v3.0
        description: "A smaller, faster version of embed-english-v3.0. Almost as capable, but a lot faster. English only."

      - name: cohere.embed-multilingual-light-v3.0
        model_id: cohere.embed-multilingual-light-v3.0
        description: "A smaller, faster version of embed-multilingual-v3.0. Almost as capable, but a lot faster. Supports multiple languages."

    dedicated:
      - name: my-dedicated-model-name
        endpoint: https://ocid1.generativeaiendpoint....  # endpoint url for dedicated model
        description: "my dedicated model description"

    datascience:
      - name: my-datascience-model-name
        endpoint: https://modeldeployment.xxxxxx/predict  # Model deployment endpoint url
        description: "my dedicated model description"

- region: us-ashburn-1
  compartment_id: ocid1.compartment.oc1..xxxxxxxxxxxxxxxxxxxxxxxxxxxxxxx
  models:
    datascience:
      - name: ODSC-Mistral-7B-Instruct
        endpoint: https://modeldeployment.us-ashburn-1.oci.customer-oci.com/ocid1.datasciencemodeldeployment.oc1.iad.xxxxxxxxxxxxxxxxxxxxxxxxxxxxxxx/predict
        description: "Data science Model deployment for Mistral 7B Instruct model"
      - name: ODSC-DeepSeek-R1-Distill-Qwen-7B
        endpoint: https://modeldeployment.us-ashburn-1.oci.customer-oci.com/ocid1.datasciencemodeldeployment.oc1.iad.xxxxxxxxxxxxxxxxxxxxxxxxxxxxxxx/predict
        description: "Data science Model deployment for DeepSeek-R1-Distill-Qwen-7B model"

# Modify this file to specify the call information of the model.
# You can define 3 types of models:
# ondemand: pre-trained model on OCI generative AI
# dedicated: dedicated model on OCI generative AI, including dedicated infrastructure, fine-tuned model, etc.
# datascience: model deployed by OCI data science service

# Where:
# region: the region where the model is located
# compartment_id: the compartment where the model is located
# name: any specified model name, use this name to point to different models when calling
# model_id: for ondemand, it is the model id, for dedicated and datascience, it is the call endpoint
